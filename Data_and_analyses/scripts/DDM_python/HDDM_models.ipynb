{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#check requirement.txt file for correct package versioning. You will very likely run into issues if you have discrepant package versions\n",
    "#see also: http://ski.clps.brown.edu/hddm_docs/tutorial.html for correct set up. I used Conda to create the virtual environment for this project.\n",
    "#I ran into many issues trying to get this project to run on M1 macbook and reccomend you do not attempt\n",
    "#Warning, this script takes a long time to run! You can reduce the # of samples to speed up the process, although this will have other consequences that might not be ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/cleaned/ddm_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-feac1290300e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSeries\u001B[0m  \u001B[0;31m# to manipulate data-frames generated by hddm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m20000\u001B[0m \u001B[0;31m#of samples for MCMC\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhddm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/data/cleaned/ddm_data.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#to ensure all looks right\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/hddm_2021_12/lib/python3.6/site-packages/kabuki/utils.py\u001B[0m in \u001B[0;36mload_csv\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    132\u001B[0m     \u001B[0;34m:\u001B[0m\u001B[0mSeeAlso\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0msave_csv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m     \"\"\"\n\u001B[0;32m--> 134\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/hddm_2021_12/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[1;32m    686\u001B[0m     )\n\u001B[1;32m    687\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 688\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    689\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    690\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/hddm_2021_12/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 454\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    455\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/hddm_2021_12/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    946\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    947\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 948\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    949\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    950\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/hddm_2021_12/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1178\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1179\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"c\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1180\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1181\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1182\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/hddm_2021_12/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   2008\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"usecols\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2009\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2010\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2011\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2012\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/data/cleaned/ddm_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hddm\n",
    "import numpy as np\n",
    "from patsy import dmatrix  # for generation of (regression) design matrices\n",
    "import os\n",
    "import earthpy as et #some helpful functions for path nav\n",
    "import seaborn as sns\n",
    "from pandas import Series  # to manipulate data-frames generated by hddm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make sure to save the project in home directory and then run below code to set correct directory for project\n",
    "os.chdir(os.path.join(et.io.HOME, 'Computational_Mech_manuscript'))\n",
    "os.getcwd()\n",
    "samples = 20000 #of samples for MCMC\n",
    "data = hddm.load_csv('./data/cleaned/ddm_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#link function if needed\n",
    "def z_link_func(x, data=data):\n",
    "    stim = (np.asarray(dmatrix('0 + C(s, [[0], [1]])',\n",
    "                              {'s': data.condition.loc[x.index]},return_type='dataframe'))\n",
    "    )\n",
    "    # Apply z = (1 - x) to flip them along 0.5\n",
    "    z_flip = np.subtract(stim, x.to_frame())\n",
    "    # The above inverts those values we do not want to flip,\n",
    "    # so invert them back\n",
    "    z_flip[stim == 0] *= -1\n",
    "    return z_flip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#custom ppc function generating 500 simulated data sets the size equal to #of trials and participants\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pymc.progressbar as pbar\n",
    "def _parents_to_random_posterior_sample(bottom_node, pos=None):\n",
    "    \"\"\"Walks through parents and sets them to pos sample.\"\"\"\n",
    "    for i, parent in enumerate(bottom_node.extended_parents):\n",
    "        if not isinstance(parent, pm.Node): # Skip non-stochastic nodes\n",
    "            continue\n",
    "\n",
    "        if pos is None:\n",
    "            # Set to random posterior position\n",
    "            pos = np.random.randint(0, len(parent.trace()))\n",
    "\n",
    "        assert len(parent.trace()) >= pos, \"pos larger than posterior sample size\"\n",
    "        parent.value = parent.trace()[pos]\n",
    "def _post_pred_generate(bottom_node, samples=500, data=None, append_data=True):\n",
    "    \"\"\"Generate posterior predictive data from a single observed node.\"\"\"\n",
    "    datasets = []\n",
    "    ##############################\n",
    "    # Sample and generate stats\n",
    "    for sample in range(samples):\n",
    "        _parents_to_random_posterior_sample(bottom_node)\n",
    "        # Generate data from bottom node\n",
    "        sampled_data = bottom_node.random()\n",
    "        if append_data and data is not None:\n",
    "            sampled_data.reset_index(inplace=True)  # Only modification of original Kabuki code\n",
    "            sampled_data = sampled_data.join(data.reset_index(), lsuffix='_sampled')\n",
    "        datasets.append(sampled_data)\n",
    "    return datasets\n",
    "def post_pred_gen(model, groupby=None, samples=500, append_data=False, progress_bar=True):\n",
    "    results = {}\n",
    "\n",
    "    # Progress bar\n",
    "    if progress_bar:\n",
    "        n_iter = len(model.get_observeds())\n",
    "        bar = pbar.progress_bar(n_iter)\n",
    "        bar_iter = 0\n",
    "    else:\n",
    "        print(\"Sampling...\")\n",
    "\n",
    "    if groupby is None:\n",
    "      iter_data = ((name, model.data.iloc[obs['node'].value.index]) for name, obs in model.iter_observeds())\n",
    "\n",
    "    else:\n",
    "        iter_data = model.data.groupby(groupby)\n",
    "\n",
    "    for name, data in iter_data:\n",
    "        node = model.get_data_nodes(data.index)\n",
    "\n",
    "        if progress_bar:\n",
    "            bar_iter += 1\n",
    "            bar.update(bar_iter)\n",
    "\n",
    "        if node is None or not hasattr(node, 'random'):\n",
    "            continue # Skip\n",
    "\n",
    "        ##############################\n",
    "        # Sample and generate stats\n",
    "        datasets = _post_pred_generate(node, samples=samples, data=data, append_data=append_data)\n",
    "        results[name] = pd.concat(datasets, names=['sample'], keys=list(range(len(datasets))))\n",
    "\n",
    "    if progress_bar:\n",
    "        bar_iter += 1\n",
    "        bar.update(bar_iter)\n",
    "\n",
    "    return pd.concat(results, names=['node'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Z & V model: Bias in both the starting point and drift rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bias in both starting point and drift rate\n",
    "v_reg = {'model': 'v ~ 1 + condition + stim_n + stim_weight',  'link_func': lambda x: x}\n",
    "z_reg = {'model': 'z ~ 1 ',  'link_func': lambda x: x}\n",
    "reg_descr = [v_reg, z_reg]\n",
    "FullModel = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "FullModel.find_starting_values()\n",
    "FullModel.sample(samples, burn=samples/10, thin=2, dbname='FullModel_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check convergence using gelman-rubin\n",
    "#this model uses the reg_descr defined above, be sure to run in order\n",
    "from kabuki.analyze import gelman_rubin\n",
    "models = []\n",
    "for i in range(5):\n",
    "    m = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=2)\n",
    "    models.append(m)\n",
    "gelman_rubin(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check posterior convergence via chains\n",
    "FullModel.plot_posteriors()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot posteriors. This appears in SOM\n",
    "z_int,v_int, v_cond =  FullModel.nodes_db.loc[[\n",
    "                                              \"z_Intercept\",\n",
    "                                              \"v_Intercept\", \"v_condition\"], 'node']\n",
    "hddm.analyze.plot_posterior_nodes([z_int])\n",
    "plt.xlabel('Starting point')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('posteriors of within-subject starting point effects.')\n",
    "hddm.analyze.plot_posterior_nodes([v_int])\n",
    "plt.xlabel('Drift-rate when outgroup is better')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('posteriors of within-subject drift-rate effects.')\n",
    "hddm.analyze.plot_posterior_nodes([v_cond])\n",
    "plt.xlabel('Drift-rate when ingroup is better')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('posteriors of within-subject drift-rate effects.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this chunk is for saving the model outputs (e.g., traces). Uncomment to run\n",
    "#save subj parms\n",
    "# stats = FullModel.gen_stats()\n",
    "# stats.to_csv('saved_hddm_models_and_parms/model_outputs/subject_parms/')\n",
    "# #save model traces\n",
    "# a, t = FullModel.nodes_db.node[['a', 't']]\n",
    "# z_0, v_0,v_m,v_s_n, v_s_w = FullModel.nodes_db.node[['z_Intercept','v_Intercept', 'v_condition', 'v_stim_n', 'v_stim_weight']]\n",
    "# allParms = a.trace()\n",
    "# allParms = np.column_stack([allParms,t.trace()])\n",
    "# allParms = np.column_stack([allParms,z_0.trace()])\n",
    "# allParms = np.column_stack([allParms,v_0.trace(),v_m.trace(),v_s_n.trace(),v_s_w.trace()])\n",
    "# np.savetxt('saved_hddm_models_and_parms/model_outputs/trace_processed/', allParms , delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bias only in starting point but not drift rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#fit bias starting point model as indicated with bias=T function\n",
    "v_reg = {'model': 'v ~ 1 + stim_n + stim_weight',  'link_func': lambda x: x}\n",
    "#z_reg = {'model': 'z ~ 1 ',  'link_func': lambda x: x}\n",
    "reg_descr = [v_reg]\n",
    "SimpleBias = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "SimpleBias.find_starting_values()\n",
    "SimpleBias.sample(samples, burn=samples/10, thin=2, dbname='simpleBias_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check convergence\n",
    "#this model takes in the reg_descr defined above so be sure to run in order\n",
    "from kabuki.analyze import gelman_rubin\n",
    "models = []\n",
    "for i in range(5):\n",
    "    m = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=2)\n",
    "    models.append(m)\n",
    "\n",
    "gelman_rubin(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check convergence via chains, the SD is often less stable.\n",
    "SimpleBias.plot_posteriors()\n",
    "#plot posteriors\n",
    "z_int= SimpleBias.nodes_db.loc[[\"z_Intercept\"], 'node']\n",
    "hddm.analyze.plot_posterior_nodes([z_int])\n",
    "plt.xlabel('Starting point')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('posteriors of within-subject starting point effects.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save subj parms\n",
    "# stats = simpleBias.gen_stats()\n",
    "# stats.to_csv('saved_hddm_models_and_parms/model_outputs/subject_parms')\n",
    "\n",
    "#Save model traces\n",
    "# a, t = SimpleBias.nodes_db.node[['a', 't']]\n",
    "# z_0 = SimpleBias.nodes_db.node[[ 'z_Intercept']]\n",
    "# v_s_n,v_s_w,v_0 = SimpleBias.nodes_db.node[['v_stim_n','v_stim_weight','v_Intercept']]\n",
    "# allParms = a.trace()\n",
    "# allParms = np.column_stack([allParms,t.trace()])\n",
    "# allParms = np.column_stack([allParms,z_0.trace()])\n",
    "# allParms = np.column_stack([allParms,v_s_n.trace(),v_s_w.trace(),v_0.trace()])\n",
    "# np.savetxt('saved_hddm_models_and_parms/model_outputs/trace_processed', allParms , delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bias only in drift rate but not starting point"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#no bias in starting point is indicated with bias=F\n",
    "v_reg = {'model': 'v ~ 1 + condition+ stim_n + stim_weight',  'link_func': lambda x: x}\n",
    "reg_descr = [v_reg]\n",
    "BiasDrift = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=False, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "BiasDrift.find_starting_values()\n",
    "BiasDrift.sample(samples, burn=samples/10, thin=2, dbname='BiasDrift_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check convergence\n",
    "from kabuki.analyze import gelman_rubin\n",
    "models = []\n",
    "for i in range(5):\n",
    "    m = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=2)\n",
    "    models.append(m)\n",
    "gelman_rubin(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check posterior convergence via chains\n",
    "BiasDrift.plot_posteriors()\n",
    "#plot posteriors\n",
    "v_int, v_cond =  BiasDrift.nodes_db.loc[[\"v_Intercept\", \"v_condition\"], 'node']\n",
    "hddm.analyze.plot_posterior_nodes([v_int])\n",
    "plt.xlabel('Drift-rate when outgroup is better')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('posteriors of within-subject drift-rate effects.')\n",
    "hddm.analyze.plot_posterior_nodes([v_cond])\n",
    "plt.xlabel('Drift-rate when ingroup is better')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('posteriors of within-subject drift-rate effects.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save subj parms\n",
    "# stats = BiasDrift.gen_stats()\n",
    "# stats.to_csv('saved_hddm_models_and_parms/model_outputs/subject_parms')\n",
    "# # #Save model traces\n",
    "# a, t = BiasDrift.nodes_db.node[['a', 't']]\n",
    "# z_0,v_m  = BiasDrift.nodes_db.node[['v_Intercept', 'v_condition']]\n",
    "# v_s_n,v_s_w = BiasDrift.nodes_db.node[['v_stim_n','v_stim_weight']]\n",
    "# allParms = a.trace()\n",
    "# allParms = np.column_stack([allParms,t.trace()])\n",
    "# allParms = np.column_stack([allParms,z_0.trace(),v_m.trace()])\n",
    "# allParms = np.column_stack([allParms,v_s_n.trace(),v_s_w.trace()])\n",
    "# np.savetxt('saved_hddm_models_and_parms/model_outputs/trace_processed', allParms , delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Null Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#fit model where starting point is fixed and no difference in drift rate based on condition\n",
    "v_reg = {'model': 'v ~ 1 + stim_n + stim_weight', 'link_func': lambda x: x}\n",
    "reg_descr = [v_reg]\n",
    "NullModel= hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=False, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "NullModel.find_starting_values()\n",
    "NullModel.sample(samples, burn=samples/10, thin=2, dbname='NullModel_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check convergence\n",
    "from kabuki.analyze import gelman_rubin\n",
    "models = []\n",
    "for i in range(5):\n",
    "    m = hddm.models.HDDMRegressor(data, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=2)\n",
    "    models.append(m)\n",
    "gelman_rubin(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check posterior convergence via chains\n",
    "NullModel.plot_posteriors()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DIC model fit indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print model fit DIC\n",
    "print(\"full Model DIC: %f\" % FullModel.dic)\n",
    "print(\"full Model DIC: %f\" % SimpleBias.dic)\n",
    "print(\"full Model DIC: %f\" % BiasDrift.dic)\n",
    "print(\"full Model DIC: %f\" % NullModel.dic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Run Posterior predictive checks\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##generate posterior data for each model\n",
    "PPC_FullModel = post_pred_gen(FullModel, append_data=True)\n",
    "PPC_SimpleBias = post_pred_gen(SimpleBias, append_data=True)\n",
    "PPC_BiasDrift = post_pred_gen(BiasDrift, append_data=True)\n",
    "PPC_NullModel = post_pred_gen(NullModel, append_data=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##compare each model's simulated data to OG data\n",
    "ppc_compare_Full = hddm.utils.post_pred_stats(data, PPC_FullModel)\n",
    "ppc_compare_Simple = hddm.utils.post_pred_stats(data, PPC_SimpleBias)\n",
    "ppc_compare_Drift = hddm.utils.post_pred_stats(data, PPC_BiasDrift)\n",
    "ppc_compare_Null = hddm.utils.post_pred_stats(data, PPC_NullModel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##print PPC\n",
    "print(ppc_compare_Full)\n",
    "print(ppc_compare_Simple)\n",
    "print(ppc_compare_Drift)\n",
    "print(ppc_compare_Null)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Parameter recovery starts here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Best fitting model (z & v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load in data simulated from best fitting model\n",
    "sim_data_sim_full = hddm.load_csv('saved_hddm_models_and_parms/param_recovery_input/FullModel_simulated.csv')\n",
    "v_reg = {'model': 'v ~ 1 + condition + stim_n + stim_weight',  'link_func': lambda x: x}\n",
    "z_reg = {'model': 'z ~ 1 ',  'link_func': lambda x: x}\n",
    "\n",
    "reg_descr = [v_reg, z_reg]\n",
    "FullModel_recovery = hddm.models.HDDMRegressor(sim_data_sim_full, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "FullModel_recovery.find_starting_values()\n",
    "FullModel_recovery.sample(samples, burn=samples/10, thin=2, dbname='FullModel_recovered_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save sub parms from best fitting recovered model\n",
    "# stats = FullModel_recovery.gen_stats()\n",
    "# stats.to_csv('saved_hddm_models_and_parms/param_recovery_output')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bias starting point model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load in data simulated from biased starting point model\n",
    "sim_data_Bias = hddm.load_csv('saved_hddm_models_and_parms/param_recovery_input/SimpleBias_simulated.csv')\n",
    "v_reg = {'model': 'v ~ 1 + stim_n + stim_weight',  'link_func': lambda x: x}\n",
    "z_reg = {'model': 'z ~ 1 ',  'link_func': lambda x: x}\n",
    "reg_descr = [v_reg, z_reg]\n",
    "simpleBias_recovery = hddm.models.HDDMRegressor(sim_data_Bias, reg_descr,\n",
    "                                       bias=True, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "simpleBias_recovery.find_starting_values()\n",
    "simpleBias_recovery.sample(samples, burn=samples/10, thin=2, dbname='simpleBias_recovered_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save sub parms from Bias drift model\n",
    "# stats = simpleBias_recovery.gen_stats()\n",
    "# stats.to_csv('saved_hddm_models_and_parms/param_recovery_output')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bias drift rate model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load in data simulated from biased drift model\n",
    "sim_data_Drift = hddm.load_csv('saved_hddm_models_and_parms/param_recovery_input/BiasDrift_simulated.csv')\n",
    "v_reg = {'model': 'v ~ 1 + condition+ stim_n + stim_weight',  'link_func': lambda x: x}\n",
    "reg_descr = [v_reg]\n",
    "BiasDrift_recovery = hddm.models.HDDMRegressor(sim_data_Drift, reg_descr,\n",
    "                                       bias=False, p_outlier=0.05,\n",
    "                                       group_only_regressors=False)\n",
    "BiasDrift_recovery.find_starting_values()\n",
    "BiasDrift_recovery.sample(samples, burn=samples/10, thin=2, dbname='BiasDrift_recovered_traces.db', db='pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save sub parms from Bias drift model\n",
    "# stats = BiasDrift_recovery.gen_stats()\n",
    "# stats.to_csv('saved_hddm_models_and_parms/param_recovery_output')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#see R script for correlations between recovered and estimates parameters\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}